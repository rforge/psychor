%\VignetteIndexEntry{An Introduction to the Gifi package}
%\VignetteEngine{knitr::knitr} 
\documentclass[10pt,nojss,nofooter,fleqn]{jss}
\usepackage{amsmath, amsfonts}
\usepackage{float,amssymb}
\usepackage{hyperref}

\newcommand{\defi}{\mathop{=}\limits^{\Delta}}  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Patrick Mair \\ Harvard University \And 
        Jan de Leeuw \\ University of California, Los Angeles \AND
        Patrick Groenen \\ Erasmus University Rotterdam
}
\title{The Gifi Package for Categorical Multivariate Analysis in \proglang{R}}
%
%%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Patrick Mair, Jan de Leeuw} %% comma-separated
\Plaintitle{The Gifi Package for Categorical Multivariate Analysis in R} %% without formatting
\Shorttitle{Gifi in \proglang{R}} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
This package vignette is an update and extension of the paper by published in the Journal of Statistical Software.
Homogeneity analysis combines the idea of maximizing the correlations between variables of a multivariate data set with
that of optimal scaling. In this article we present methodological and practical issues of the \proglang{R} package \pkg{homals} which performs homogeneity analysis and various extensions. By setting rank constraints nonlinear principal component analysis can be performed. The variables can be partitioned into sets such that homogeneity analysis is extended to nonlinear canonical correlation analysis or to predictive models which emulate discriminant analysis and regression models. For each model the scale level of the variables can be taken into account by setting level constraints. All algorithms allow for missing values.
}
\Keywords{Gifi methods, optimal scaling, homogeneity analysis, categorical PCA}
\Plainkeywords{Gifi methods, optimal scaling, homogeneity analysis, categorical PCA} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: This needs to filled out ONLY IF THE PAPER WAS ACCEPTED.
%% If it was not (yet) accepted, leave them commented.
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Patrick Mair\
  Department of Psychology\\
  Harvard University\\
  E-mail: \email{mair@fas.harvard.edu}\\
  URL: \url{http://scholar.harvard.edu/mair}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/1/31336-5053
%% Fax: +43/1/31336-734

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

<<preliminaries, echo=FALSE>>=
require(Gifi)
@

\section{Introduction}
\label{sec:int}


\section{Gifi Methods for Categorical Multivariate Analysis}
\label{sec:gifi}

\subsection{The Gifi Loss Function}
In this section we give the very general definition of the Gifi loss function. 
We restrict our elaborations to the very basic expressions. For simplicity, we are not incorporating the formulation for missing data even though the package allows for this. A more detailed and slightly more technical formulation can be found in \citet{deLeeuw+Mair:2009a}. 


Homogeneity analysis is based on the criterion of minimizing the departure from homogeneity. This departure is measured by a loss function. To write the corresponding basic equations the following definitions are needed. For $i=1,\ldots,n$ objects, data on $m$ (categorical) variables are collected in a data matrix $\mathbf H$ of dimension $n \times m$. 
Each of the $j=1,\ldots,m$ variable takes on $k_j$ different values (their \emph{levels} or \emph{categories}). We code them using $n\times k_j$ binary \emph{indicator matrices} $\mathbf G_j$, i.e., a matrix of dummy variables for each variable. The whole set of indicator matrices can be collected in a block matrix
\begin{equation}
\mathbf G\defi\begin{bmatrix}\mathbf G_1&\vdots&\mathbf G_2&\vdots&\cdots&\vdots&\mathbf G_m\end{bmatrix}.
\end{equation}
For convenience we introduce $\mathbf D_j^{}=\mathbf G_j'\mathbf G_j^{}$ as the $k_j\times k_j$ diagonal matrix with the (marginal) frequencies of variable $j$ in its main diagonal. 

Let $\mathbf X$ be the unknown $n\times p$ matrix containing the coordinates (\emph{object scores}) of the object projections into $\mathbb{R}^p$. Furthermore, let $\mathbf Y_j$ be the unknown $k_j \times p$ matrix containing the coordinates of the category projections into the same $p$-dimensional space (\emph{category quantifications}). 
The problem of finding these solutions can be formulated by means of the following sum-of-squares (SSQ) loss function to be minimized:
\begin{equation}
\label{eq:loss}
\sigma(\mathbf X;\mathbf Y_1,\ldots,Y_m) = \frac{1}{m} \sum_{j=1}^m\mathbf{tr}(\mathbf X-\mathbf G_j\mathbf Y_j)'
(\mathbf X-\mathbf G_j\mathbf Y_j)
\end{equation}
We use the normalization $\mathbf X'\mathbf X=n\mathbf I$ in order to avoid the trivial solution $\mathbf X=\mathbf 0$ 
and $\mathbf Y_j=\mathbf 0$. Note that from an analytical point of view the loss function represents the sum-of-squares of 
$(\mathbf X-\mathbf G_j\mathbf Y_j)$ which obviously involves the object scores and the category quantifications. Thus, we minimize simultaneously over $\mathbf X$ and $\mathbf Y_j$. 

This minimization problem is solved by the iterative \emph{alternating least squares algorithm} (ALS; sometimes quoted as \emph{reciprocal averaging algorithm}). At iteration $t=0$ we start with arbitrary object scores $\mathbf X^{(0)}$. Each iteration $t$ consists of three steps:
\begin{enumerate}
\item Update category quantifications: $\mathbf Y_j^{(t+1)}=\mathbf D_j^{-1}\mathbf G_j'\mathbf X^{(t)}$ for $j=1,\ldots,m$
\item Update object scores: $\tilde{\mathbf X}^{(t+1)}=\sum_{j=1}^m \mathbf G_j^{}\mathbf Y_j^{(t+1)}$
\item Normalization: $X^{(t+1)}= \tilde{\mathbf X}^{(t+1)}\left(\tilde{\mathbf X}^{(t+1)}{'}\tilde{\mathbf X}^{(t+1)}\right)^{-\frac{1}{2}}\sqrt{n}$
\end{enumerate}
The algorithm stops when the decrease in the loss function gets below a pre-specified level $\varepsilon$. 

A core output of the Gifi optimization is a set of $n \times m$ score matrices 
for each dimension. These matrices contain the rescaled category scores instead of the observed categories. 
Typically, the one of the first dimension denoted as $\mathbf H^{\ast}$ is the most important one.  
It can be used for subsequent statistical analyses and replaces the original data matrix $\mathbf{H}$.

\subsection{Optimal Scaling}
\label{sec:opt}

\subsection{Coding Schemes for the Indicator Matrix}

\section{Categorical Principal Component Analysis}
\label{sec:princals}
\subsection{Nonlinear PCA Theory}
From a homogeneity analysis perspective, i.e. in analogy to equation (\ref{eq:loss}), the PCA problem can be stated as follows: 
\begin{equation}
\label{eq:losspc}
\sigma(\mathbf X;\mathbf A) = = \frac{1}{m} \sum_{j=1}^m 
\mathbf{tr}(\mathbf X - \mathbf h_j\mathbf a_j')'(\mathbf X - \mathbf h_j\mathbf a_j')
\end{equation}

with $\mathbf X$ as $n \times p$ matrix and $\mathbf A$ as $m \times p$ matrix. The vectors $\mathbf a_j'$ are the rows 
of $\mathbf A$ containing the weights for variable $j$; $\mathbf h_j$ are simply the columns of the data matrix $\mathbf H$. 
We see that $\mathbf h_j\mathbf a_j'$ reflects a linear weighting of the observed values $\mathbf h_j$ (i.e. a linear transformation). Minimizing (\ref{eq:losspc}) means computing the $p$ principal components. 

A strinking feature of the Gifi framework is that we have several possibilities to transform each variable individually in a nonlinear way through optimal scaling. Let $\phi_j(\cdot)$ denote the corresponding transformation function. Incorporating, 
$\phi(\cdot)$ into (\ref{eq:losspc}) gives

\begin{equation}
\label{eq:losspc}
\sigma(\mathbf X;\mathbf A) = = \frac{1}{m} \sum_{j=1}^m 
\mathbf{tr}(\mathbf X - \phi_j(\mathbf h_j)\mathbf a_j')'(\mathbf X - \phi_j(\mathbf h_j)\mathbf a_j')
\end{equation}

The weights $\mathbf a_j$ can only be identified when $\phi_j(\mathbf h_j)$ is normalized. Since these two quantities are 
confounded we can subsume $\mathbf a_j$ in $\phi_j(\mathbf h_j)$. In Gifi, $\mathbf H$ is categorical and the minimization does not operate directly on the raw variables $\mathbf h_j$. Rather, it operates on the corresponding indicator matrix $\mathbf G_j$. 
Therefore  $\phi_j(\mathbf h_j) = \mathbf G_j\mathbf y_j$ where $\mathbf y_j$ is a vector of single category 
quantifications (rather then using the original $\mathbf h_j$). We can express (\label{eq:losspc}) as

\begin{equation}
\label{eq:loss1}
\sigma(\mathbf X;\mathbf y_1,\ldots,y_m) = \frac{1}{m} \sum_{j=1}^m\mathbf{tr}(\mathbf X-\mathbf G_j\mathbf y_j)'
(\mathbf X-\mathbf G_j\mathbf y_j).
\end{equation}

If we compare equation (\ref{eq:loss1}) with the general Gifi loss function in (\ref{eq:loss}) we see that in (\ref{eq:loss}) we have the matrix $\mathbf Y_j$ of dimension $k_j \times p$. In PCA, however we want to have a single weighting 
scheme. This can be achieved by posing a rank-1 restriction on the matrix $\mathbf Y_j$. We can do this by applying
a simple linear weighting where $\mathbf z_j$ denotes the vector of single category quantifications and $\mathbf a_j$ 
the vector of weights which we call the PRINCALS loadings. Therefore, the rank-1 restriction can be expressed as $\mathbf Y_j = \mathbf z_j\mathbf a_j'$ and we have reconstructed our general Gifi loss function through the rank-1 restricted PRINCALS version. 
Consequently, PRINCALS can be estimated through the general Gifi framework. 

Note that if we were to apply a numerical (i.e. linear) transformation on the observed categories in $j$, minimizing 
(\ref{eq:loss1}) means computing a PCA in a numerical way. Since the optimal scaling is performed for 
each variable individually, PRINCALS allows for mixed scale levels of the input data. 

\subsection{PRINCALS vs. PCA}
Standard PCA was developed for metric variables and assumes that the relationships between the variables are linear.
PCA can be solved either by a SVD on the centered data matrix with each variable divided by $\sqrt{n-1}$, or  
by an eigenvalue decomposition of the correlation (or covariance) matrix. That is, 
$\mathbf{H}_c = \mathbf P\boldsymbol{\Lambda} \mathbf Q'$ or 
$\mathbf R(\mathbf H) = \mathbf P \boldsymbol{\Lambda}^2 \mathbf P'$ where $\boldsymbol{\Lambda}^2$ is an $m \times m$ diagonal matrix with the eigenvalues in the diagonal. 

The eigenvalues in $\boldsymbol{\Lambda}^2$ represent the variances of the principal component scores and the eigenvectors in the $m \times m$ matrix $\mathbf P$ the loadings. The principal component scores are then simply given by $\mathbf P\boldsymbol{\Lambda}$. give the vectors of weights (loadings). PCA is targeted on explaining variance and typically we are aiming to find a satisfactory solution (in terms of the explained amount of variance) in a low-dimensional space $p$. 

We see standard PCA solves the dimension reduction problem analytically, PRINCALS numerically. PCA gives eigenvalues, 
loadings, and principal component scores; PRINCALS gives object scores, category quantifications, loadings, and the score matrix. 
At this point we show how to ``translate'' the PRINCALS outputs into a PCA output. The starting point 
is the score matrix $\mathbf H^{\ast}$ of the first dimension. First, we compute the correlation matrix 
$\mathbf R^(\mathbf H^{\ast})$. Second, we perform an eigenvalue decomposition on the correlation matrix which 
gives the $m \times m$ matrix of eigenvectors $\mathbf P$ and the diagonal matrix $\boldsymbol{\Lambda}^2$ containing 
the eigenvalues. 

Note that in standard PCA the loadings are normalized to $\|\mathbf a_j\|^2 = 1$. In order to make the 
PRINCALS loadings $\mathbf a_j$ comparable to standard PCA, they need to be normalized the same way.
The \code{princals()} function performs all these computations internally and returns the $p$ eigenvalues based on the 
$\mathbf R^(\mathbf H^{\ast})$, the variance accounted for (VAF) by each dimensions (based on the eigenvalues), 
the standardized loadings, and the principal component scores. 

Note that a difference between PCA and PRINCALS is that the PRINCALS solutions for varying dimensionality $p$ are not nested. 
In PCA they are, of course. If we want to compare the gain of a PRINCALS solution over a PCA we can compute the respective 
eigenvalue ratios. This gives us a measure for the violations of equidistance and linearity in our original data. 


\subsection{Rotation and Goodness-Of-Fit}
The package also offers options for rotating a after fitting a PRINCALS solution. Note that ``rotation'' is a concept developed within factor analysis (FA) by transforming the factor loadings through a rotation matrix and computing the corresponding rotated factor scores afterwards. Strictly speaking, PCA eigenvectors of the correlation/covariance matrix are not loadings (even though we just called them ``loadings'') above. Loadings within an FA context are eigenvectors scaled by the square roots of the corresponding eigenvalues. 

Therefore, if we want to rotate a PRINCALS solution, the loadings need to be re-scaled such that they are loadings in the FA sense. This applies to PRINCALS as well as to PCA. The package offers Varimax and Promax rotations and does the corresponding loadings 
normalization internally. In order to get the rotated principal scores, we multiply unrotated matrix 
of the PRINCALS principal component scores with the generalized inverse of the rotated loadings matrix. 

In order to determine the number of components, \emph{very simple structure} (VSS) analysis as well as parallel 
analysis based on random (or permuted) data matrices can be performed for PRINCALS in the same way as for PCA. 

\subsection{Example: ABC Company}
In many fields researchers often have to deal with categorical variables such as Likert items in the Social Sciences. Running a PCA on a set of items implies treating them as metric, i.e. the distances between categories (i.e. 1--2, 2--3, 3--4, etc.) have to be constant across categories. Together with the linear relationship assumption, these requirements are often not fulfilled in practice. In addition, the concept of variance only applies to metric variables. 

If we want to treat the variables on an ordinal scale, we have two options: run an ordinal factor analysis (FA) based on polychoric correlations as implemented in the \code{fa.poly()} function in the \pkg{psych} package, or run a categorical (ordinal) PCA as presented here. Apart from the conceptual differences between FA and PCA, the advantage of PRINCALS over polychoric FA is that we do not have to pose any underlying distribution assumption on our data, whereas a polychoric (or tetrachoric in the binary case) correlation assumes that the categories are realizations of an underlying latent normal distribution. 

PRINCALS does not only allow for ordinal input data, it can be also applied to mixed scale levels: some variables can be metric, 
some variables ordinal, some variables nominal. 

Through the \code{level} argument the user can specify the scale levels of the variables (\code{"ordinal" as default}. If all variables are set to \code{"numerical"}, PRINCALS mimics standard PCA. In terms of plotting possibilities, a generic plot function allows for a loadings plot (default), a scree plot, transformation plots, and a biplot by specifying the 
\code{plot.type} argument accordingly. 

Now we show an ordinal PCA example on the ABC dataset which reproduces the analysis in \cite{Ferrari+Barbiero:2012}. 
ABC is a ficticious company which launched a customer satisfaction survey. In this analysis we use six items, 
each of them on a 5-point Likert scale, covering certain aspects of customer satisfaction: 
equipment, sales support, technical support, training, purchase, and pricing. 

First, we start with a full-dimensional PRINCALS solution and examine the scree plot.

<<>>=
ABC6 <- ABC[, 6:11]   
fitfull <- princals(ABC6, ndim = 6)
fitfull
summary(fitfull)
@

The scree plot is given in Figure \ref{fig:abcscree} including the eigenvalues from a parallel analysis
(permutation version). 

<<abcscree-plot, eval=FALSE>>=
fitpar <- parallel(fitfull)   
fitpar
summary(fitpar)
plot(fitpar)
@
\begin{figure}
\begin{center}  % fig.width='15cm', fig.height='6cm'
<<abcscree-plot1, echo=FALSE, fig.width=6, fig.height=6, dev='postscript'>>=
<<abcscree-plot>>
@
\end{center}
\caption{\label{fig:abcscree} Scree plot including eigenvalues from parallel analysis.}
\end{figure}

In addition, we run a VSS. Note that if the input model for \code{parallel()} and \code{vss()} does not 
represent a full-dimensional solution, the full-dimensional models is fitted internally.  
 
<<>>=
fitvss <- vss(fitfull)   
fitvss
summary(fitvss)
@

Now we fit a two-dimensional ordinal solution and produce a loadings plot and a biplot (see \ref{fig:loadbi})

<<loadbi-plot, eval=FALSE>>=
fit2d <- princals(ABC6, ndim = 2)
fit2d
summary(fit2d)
op <- par(mfrow = c(1,2))
plot(fit2d, "loadplot", main - "Loadings Plot ABC Data")
plot(fit2d, "biplot", labels.scores = TRUE, main = "Biplot ABC Data")
par(op)
@
\begin{figure}
\begin{center}  % fig.width='15cm', fig.height='6cm'
<<loadbi-plot1, echo=FALSE, fig.width=10, fig.height=5, dev='postscript'>>=
<<loadbi-plot>>
@
\end{center}
\caption{\label{fig:loadbi} Loadings plot and biplot for ABC dataset.}
\end{figure}

We see that we account for \Sexpr{round(sum(fit2d$vaf), 2)} of the variance.

Now we fit a one-dimensional ordinal PCA, and compare the eigenvalue to the one from a standard PCA. 
<<>>=
fit1d <- princals(ABC6, ndim = 1)
ABC6m <- sapply(ABC6, function(x) as.numeric(levels(x))[x])
fitpc <- princomp(ABC6m)
fit1d$eigenvalues/(fitpc$sdev^2)[1]       ## eigenvalue ratio PRINCALS/PCA
@

The eigenvalue ratio of \Sexpr{round(fit1d$eigenvalues/(fitpc$sdev^2)[1], 2)} suggests a slight improvement 
of PRINCALS over standard PCA. This implies that the response categories are approximately equidistant and 
the relationship between the variables is not far from linear.
If we want to mimic standard PCA with PRINCALS, we declare all the variables as numeric.

<<>>=
fit1dm <- princals(ABC6, ndim = 1, level = "numerical")
fit1dm$eigenvalues/(fitpc$sdev^2)[1]       ## eigenvalue ratio NLPCA/PCA
@
The size of the eigenvalue ratio decreased since we are essentially doing the same thing. 


Finally, for illustration, we include a binary variable (nominal) into the dataset and specify two of the remaining variables
as nominal, two of them as ordinal, and two of them as metric. The transformation plot in Figure \ref{fig:transplot}
shows nicely the optimal scaling transformations of the original categories. 

<<transplot-plot, eval=FALSE>>=
ABC7 <- ABC[,c(3, 6:11)]
fit7 <- princals(ABC7, ndim = 2, 
        level = c("nominal", rep(c("nominal", "ordinal", "numerical"), each = 2)))
plot(fit7, "transplot")
@
\begin{figure}
\begin{center}  
<<transplot-plot1, echo=FALSE, fig.width=10, fig.height=10, dev='postscript'>>=
<<transplot-plot>>
@
\end{center}
\caption{\label{fig:transplot} Transformation plots.}
\end{figure}

 

% 
% \subsection{Predictive models and canonical correlation}
% \label{sec:pmcca}
% The \code{sets} argument allows for partitioning the variables into sets in order to emulate canonical correlation analysis and predictive models. As outlined above, if the variables are partitioned into asymmetric sets of one variable vs. the others, we can put this type of homals model into a predictive modeling context. If not, the interpretation in terms of canonical correlation is more appropriate. 
% 
% \begin{figure}[hbt]
% \begin{center}
% \includegraphics[height=75mm, width=75mm]{galoVor.pdf}
% \includegraphics[height=75mm, width=75mm]{galoLab.pdf}
% \caption{\label{fig:vor}Voronoi Plot and Label Plot for Galo Data}
% \end{center}
% \end{figure}
% 
% To demonstrate this, we use the \code{galo} dataset \citep{Peschar:75} where data of 1290 school children in the sixth grade of an elementary school in the city of Groningen (Netherlands) were collected. The variables are Gender, IQ (categorized into 9 ordered categories), Advice (teacher categorized the children into 7 possible forms of secondary education, i.e., Agr = agricultural; Ext = extended primary education; Gen = general; 
% Grls = secondary school for girls; Man = manual, including housekeeping; None = no further education; Uni = pre-University), SES (parent's profession in 6 categories) and School (37 different schools). In this example it could be of interest to predict Advice from Gender, IQ, and SES (whereas School is inactive).
% 
% <<>>=
% data("galo")
% res <- homals(galo, active = c(rep(TRUE, 4), FALSE), sets = list(c(1,2,4),3,5))
% plot(res, plot.type = "vorplot", var.subset = 3, asp = 1)
% plot(res, plot.type = "labplot", var.subset = 2, asp = 1)
% predict(res)
% @
% 
% A rate of .6310 correctly classified teacher advice results. The Voronoi plot in Figure \ref{fig:vor} shows the Voronoi regions for the same variable. A labeled plot is given for the IQs which shows that on the upper half of the horseshoe there are mainly children with IQ-categories 7-9. Distinctions between these levels of intelligence are mainly reflected by Dimension 1. For the lower horseshoe half it can be stated that both dimensions reflect differences in lower IQ-categories.
% 
% Using the classical iris dataset, the aim is to predict Species from Petal/Sepal Length/Width. The polynomial level constraint is posed on the predictors and the response is treated as nominal. A hull plot for the response, a label plot Petal Length and loss plots for all predictors are produced.
% 
% <<eval = FALSE>>=
% data("iris")
% res <- homals(iris, sets = list(1:4, 5), level = c(rep("polynomial", 4), "nominal"), rank = 2, itermax = 2000)
% plot(res, plot.type = "hullplot", var.subset = 5, cex = 0.7, xlim = c(-3,3), ylim = c(-4,3), asp = 1)
% plot(res, plot.type = "labplot", var.subset = 3, cex = 0.7, xlim = c(-3,3), ylim = c(-4,3), asp = 1)
% @
% 
% \begin{figure}[hbt]
% \begin{center}
% \includegraphics[height=75mm, width=75mm]{irisHull.pdf}
% \includegraphics[height=75mm, width=75mm]{irisLab.pdf}
% \caption{\label{fig:iris}Hullplot and Label Plot for Iris Data}
% \end{center}
% \end{figure}
% 
% For this two-dimensional homals solution, 100\% of the iris species are correctly classified. The hullplot in Figure \ref{fig:iris} shows that the species are clearly separated on the two-dimensional plane. In the label plot the object scores are labeled with the response on Petal Length and it becomes obvious that small lengths form the setosa ``cluster", whereas iris virginica are composed by obervations with large petal lengths. Iris versicolor have medium lengths. 
% 
% The loss plots in Figure \ref{fig:irisLoss} show the fitted rank-2 solution (red lines) against the unrestricted solution. The implication of the polynomial level restriction for the fitted model is obvious.
% 
% <<eval = FALSE>>=
% plot(res, plot.type = "lossplot", var.subset = 1:4, cex = 0.7, xlim = c(-3, 3), ylim = c(-5, 3), asp = 1)
% @
% 
% \begin{figure}[hbt]
% \begin{center}
% \includegraphics[height=70mm, width=70mm]{irisLoss1.pdf}
% \includegraphics[height=70mm, width=70mm]{irisLoss2.pdf}
% \includegraphics[height=70mm, width=70mm]{irisLoss3.pdf}
% \includegraphics[height=70mm, width=70mm]{irisLoss4.pdf}
% \caption{\label{fig:irisLoss}Loss plots for Iris Predictors}
% \end{center}
% \end{figure}
% 
% To show another homals application of predictive (in this case regression) modeling we use the Neumann dataset \citep{Wilson:26}: Willard Gibbs discovered a theoretical formula connecting the density, the pressure, and the absolute temperature of a mixture of gases with convertible 
% components. He applied this formula and the estimated constants to 65 experiments carried out by Neumann, and he discusses the systematic and accidental divergences (residuals). In the \pkg{homals} package such a linear regression of density on temperature and pressure can be emulated by setting numerical levels. Constraining the levels to be ordinal, we get a monotone regression \citep{Gifi:90}. 
% 
% <<eval = FALSE>>=
% data("neumann")
% res.lin <- homals(neumann, sets = list(3,1:2), level = "numerical", rank = 1)
% res.mon <- homals(neumann, sets = list(3,1:2), level = "ordinal", rank = 1)
% plot(res.lin, plot.type = "loadplot", main = "Loadings Plot Linear Regression", xlim = c(-10, 10), ylim = c(-5, 10), asp = 1)
% plot(res.mon, plot.type = "loadplot", main = "Loadings Plot Monotone Regression", xlim = c(-10, 10), ylim = c(-5, 10), asp = 1)
% @
% 
% The points in the loadings plot in Figure \ref{fig:neuload} correspond to regression coefficients.
% 
% \begin{figure}[hbt]
% \begin{center}
% \includegraphics[height=70mm, width=70mm]{neuloadlin.pdf}
% \includegraphics[height=70mm, width=70mm]{neuloadmon.pdf}
% \caption{\label{fig:neuload}Loading Plots for Neumann Regression}
% \end{center}
% \end{figure}
% 
% The impact of the level restrictions on the scaling is visualized in the transformation plots in Figure \ref{fig:neutrf}. Numerical level restrictions lead to linear transformations of the original scale with respect to the homals scaling (i.e. linear regression). Pertaining to ordinal levels, monotone transformations are carried out (i.e. monotone regression). 
% 
% \begin{figure}[hbt]
% \begin{center}
% \includegraphics[height=60mm, width=60mm]{neutrftemp.pdf}
% \includegraphics[height=60mm, width=60mm]{neutrfpres.pdf}
% \includegraphics[height=60mm, width=60mm]{neutrfdens.pdf}
% \caption{\label{fig:neutrf}Transformation Plots for Neumann Regression}
% \end{center}
% \end{figure}
% 
% \subsection{NLPCA on Roskam data}
% \label{sec:pcex}
% \citet{Roskam:68} collected preference data where 39 psychologists ranked all nine areas (see Table \ref{tab:area}) of the Psychology Department at the University of Nijmengen. 
% 
% \begin{table}[ht]
% \centering
% \begin{tabular}{|c|l|}
% \hline
% SOC& Social Psychology\\
% EDU& Educational and Developmental Psychology\\
% CLI& Clinical Psychology\\
% MAT& Mathematical Psychology and Psychological Statistics\\
% EXP& Experimental Psychology\\
% CUL& Cultural Psychology and Psychology of Religion\\
% IND& Industrial Psychology\\
% TST& Test Construction and Validation\\
% PHY& Physiological and Animal Psychology\\
% \hline
% \end{tabular}
% \caption{\label{tab:area}Psychology Areas in Roskam Data.}
% \end{table}
% 
% Using this data set we will perform two-dimensional NLPCA by restricting the rank to be 1. Note that the objects are the areas and the variables are the psychologists. Thus, the input data structure is a $9 \times 39$ data frame. Note that the scale level is set to ``ordinal".
% 
% <<eval = FALSE>>=
% data("roskam")
% res <- homals(roskam, rank = 1, level = "ordinal")
% plot(res, plot.type = "objplot", xlim = c(-2.5, 2.5), ylim = c(-2.5, 2.5), asp = 1)
% plot(res, plot.type = "vecplot", var.subset = 2, main = "Vector Plot Rater 2", xlim = c(-2.5, 2.5), ylim = c(-2.5, 2.5), asp = 1)
% @
% 
% \begin{figure}[hbt]
% \begin{center}
% \includegraphics[height=75mm, width=75mm]{roskamobj.pdf}
% \includegraphics[height=75mm, width=75mm]{roskamvec.pdf}
% \caption{\label{fig:roskam}Plots for Roskam data}
% \end{center}
% \end{figure}
% 
% The object plot in Figure \ref{fig:roskam} shows interesting rating ``twins" of departmental areas: mathematical and experimental psychology, industrial psychology and test construction (both are close to the former two areas), educational and social psychology, clinical and cultural psychology. Physiological and animal psychology are somewhat separated from the other areas. Obviously this rater is attracted to areas like social, cultural and clinical psychology rather than to methodological fields. 
% The vector plot on the right hand side projects the category scores onto a straight line determined by rank restricted category quantifications. Similarly, a projection plot could be created. Further analyses of this dataset within a PCA context can be found in \citet{deLeeuw:06}. 
 
% \section{Discussion}
% In this paper theoretical foundations of the methodology used in the \pkg{homals} package are elaborated and package application and visualization issues are presented. Basically, \pkg{homals} covers the techniques described in \citet{Gifi:90}: Homogeneity analysis, NLCCA, predictive models, and NLPCA. It can handle missing data and the scale level of the variables can be taken into account. The package offers a broad variety of real-life datasets and furthermore provides numerous methods of visualization, either in a two-dimensional or in a three-dimensional way. Future enhancements will be to replace indicator matrices by more general B-spline bases and to incorporate weights for observations. To conclude, \pkg{homals} provides flexible, easy-to-use routines which allow researchers from different areas to compute, interpret, and visualize methods belonging to the Gifi family. 

\bibliography{Gifi}

\end{document}

